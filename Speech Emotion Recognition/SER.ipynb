{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import sounddevice as sd\n",
    "import wavio\n",
    "import tensorflow as tf\n",
    "import streamlit as st\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Dataset Paths\n",
    "Crema_path = \"Documents/Speech Emotion Recognition/Crema/\"\n",
    "Ravdess_path = \"Documents/Speech Emotion Recognition/Ravdess/audio_speech_actors_01-24/\"\n",
    "Savee_path = \"Documents/Speech Emotion Recognition/Savee/\"\n",
    "Tess_path = \"Documents/Speech Emotion Recognition/Tess/\"\n",
    "\n",
    "# Emotion Mappings\n",
    "CREMA_EMOTIONS = {\"SAD\": \"sad\", \"ANG\": \"angry\", \"DIS\": \"disgust\", \"FEA\": \"fear\", \"HAP\": \"happy\", \"NEU\": \"neutral\"}\n",
    "RAVDESS_EMOTIONS = {1: \"neutral\", 2: \"calm\", 3: \"happy\", 4: \"sad\", 5: \"angry\", 6: \"fearful\", 7: \"disgust\", 8: \"surprised\"}\n",
    "SAVEE_EMOTIONS = {\"a\": \"angry\", \"d\": \"disgust\", \"f\": \"fear\", \"h\": \"happy\", \"n\": \"neutral\", \"sa\": \"sad\", \"su\": \"surprised\"}\n",
    "\n",
    "def process_crema():\n",
    "    paths, emotions = [], []\n",
    "    for file in os.listdir(Crema_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            parts = file.split(\"_\")\n",
    "            emotion = CREMA_EMOTIONS.get(parts[2], \"Unknown\")\n",
    "            paths.append(os.path.join(Crema_path, file))\n",
    "            emotions.append(emotion)\n",
    "    return pd.DataFrame({\"Path\": paths, \"Emotion\": emotions})\n",
    "\n",
    "def process_ravdess():\n",
    "    paths, emotions = [], []\n",
    "    for actor in os.listdir(Ravdess_path):\n",
    "        actor_path = os.path.join(Ravdess_path, actor)\n",
    "        if os.path.isdir(actor_path):\n",
    "            for file in os.listdir(actor_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    parts = file.split(\"-\")\n",
    "                    emotion = RAVDESS_EMOTIONS.get(int(parts[2]), \"Unknown\")\n",
    "                    paths.append(os.path.join(actor_path, file))\n",
    "                    emotions.append(emotion)\n",
    "    return pd.DataFrame({\"Path\": paths, \"Emotion\": emotions})\n",
    "\n",
    "def process_savee():\n",
    "    paths, emotions = [], []\n",
    "    for file in os.listdir(Savee_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            prefix = file[:2] if file[:2] in SAVEE_EMOTIONS else file[0]\n",
    "            emotion = SAVEE_EMOTIONS.get(prefix, \"Unknown\")\n",
    "            paths.append(os.path.join(Savee_path, file))\n",
    "            emotions.append(emotion)\n",
    "    return pd.DataFrame({\"Path\": paths, \"Emotion\": emotions})\n",
    "\n",
    "def process_tess():\n",
    "    paths, emotions = [], []\n",
    "    for folder in os.listdir(Tess_path):\n",
    "        folder_path = os.path.join(Tess_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    parts = file.split(\"_\")\n",
    "                    emotion = \"surprised\" if parts[2] == \"ps\" else parts[2].lower()\n",
    "                    paths.append(os.path.join(folder_path, file))\n",
    "                    emotions.append(emotion)\n",
    "    return pd.DataFrame({\"Path\": paths, \"Emotion\": emotions})\n",
    "\n",
    "# Load and combine datasets\n",
    "df_all = pd.concat([process_crema(), process_ravdess(), process_savee(), process_tess()], ignore_index=True)\n",
    "\n",
    "def extract_features(file_path, max_pad_length=130):\n",
    "    y, sr = librosa.load(file_path, duration=3, offset=0.5)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "\n",
    "    if mfcc.shape[1] < max_pad_length:\n",
    "        pad_width = max_pad_length - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_pad_length]\n",
    "\n",
    "    return mfcc.T\n",
    "\n",
    "X = np.array([extract_features(path) for path in df_all[\"Path\"]])\n",
    "y = df_all[\"Emotion\"]\n",
    "\n",
    "#Encode Labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = to_categorical(encoder.fit_transform(y))\n",
    "\n",
    "# Reshape input for LSTM (samples, time steps, features)\n",
    "X = np.array([x[:130, :] for x in X])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 111ms/step - accuracy: 0.3395 - loss: 1.8575 - val_accuracy: 0.5162 - val_loss: 1.2390\n",
      "Epoch 2/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 163ms/step - accuracy: 0.5299 - loss: 1.1921 - val_accuracy: 0.5750 - val_loss: 1.0938\n",
      "Epoch 3/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 117ms/step - accuracy: 0.5815 - loss: 1.0824 - val_accuracy: 0.5828 - val_loss: 1.0433\n",
      "Epoch 4/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 115ms/step - accuracy: 0.5891 - loss: 1.0571 - val_accuracy: 0.5746 - val_loss: 1.0508\n",
      "Epoch 5/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.6066 - loss: 1.0119 - val_accuracy: 0.5861 - val_loss: 1.0404\n",
      "Epoch 6/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 117ms/step - accuracy: 0.6131 - loss: 0.9829 - val_accuracy: 0.6149 - val_loss: 0.9780\n",
      "Epoch 7/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 147ms/step - accuracy: 0.6189 - loss: 0.9712 - val_accuracy: 0.6087 - val_loss: 0.9800\n",
      "Epoch 8/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.6412 - loss: 0.9091 - val_accuracy: 0.6145 - val_loss: 0.9702\n",
      "Epoch 9/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.6404 - loss: 0.9169 - val_accuracy: 0.6157 - val_loss: 0.9680\n",
      "Epoch 10/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 116ms/step - accuracy: 0.6513 - loss: 0.8910 - val_accuracy: 0.6301 - val_loss: 0.9788\n",
      "Epoch 11/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.6782 - loss: 0.8432 - val_accuracy: 0.6453 - val_loss: 0.9020\n",
      "Epoch 12/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 117ms/step - accuracy: 0.6881 - loss: 0.8122 - val_accuracy: 0.6469 - val_loss: 0.9063\n",
      "Epoch 13/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.6950 - loss: 0.8098 - val_accuracy: 0.6309 - val_loss: 0.9405\n",
      "Epoch 14/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 115ms/step - accuracy: 0.7078 - loss: 0.7918 - val_accuracy: 0.6584 - val_loss: 0.8804\n",
      "Epoch 15/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 115ms/step - accuracy: 0.7104 - loss: 0.7697 - val_accuracy: 0.6621 - val_loss: 0.8707\n",
      "Epoch 16/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 115ms/step - accuracy: 0.7160 - loss: 0.7610 - val_accuracy: 0.6663 - val_loss: 0.8541\n",
      "Epoch 17/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 117ms/step - accuracy: 0.7306 - loss: 0.7176 - val_accuracy: 0.6794 - val_loss: 0.8435\n",
      "Epoch 18/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.7265 - loss: 0.7306 - val_accuracy: 0.6683 - val_loss: 0.8652\n",
      "Epoch 19/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 116ms/step - accuracy: 0.7447 - loss: 0.6869 - val_accuracy: 0.6761 - val_loss: 0.8527\n",
      "Epoch 20/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 116ms/step - accuracy: 0.7584 - loss: 0.6438 - val_accuracy: 0.6938 - val_loss: 0.8211\n",
      "Epoch 21/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.7660 - loss: 0.6332 - val_accuracy: 0.6728 - val_loss: 0.8544\n",
      "Epoch 22/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 123ms/step - accuracy: 0.7586 - loss: 0.6606 - val_accuracy: 0.6757 - val_loss: 0.8611\n",
      "Epoch 23/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 116ms/step - accuracy: 0.7599 - loss: 0.6367 - val_accuracy: 0.6897 - val_loss: 0.8293\n",
      "Epoch 24/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 115ms/step - accuracy: 0.7703 - loss: 0.6106 - val_accuracy: 0.6749 - val_loss: 0.8753\n",
      "Epoch 25/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 117ms/step - accuracy: 0.7827 - loss: 0.5891 - val_accuracy: 0.6876 - val_loss: 0.8593\n",
      "Epoch 26/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.7821 - loss: 0.5904 - val_accuracy: 0.6942 - val_loss: 0.8446\n",
      "Epoch 27/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 115ms/step - accuracy: 0.7916 - loss: 0.5615 - val_accuracy: 0.6983 - val_loss: 0.8133\n",
      "Epoch 28/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 124ms/step - accuracy: 0.8062 - loss: 0.5250 - val_accuracy: 0.6720 - val_loss: 0.8800\n",
      "Epoch 29/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 116ms/step - accuracy: 0.7967 - loss: 0.5463 - val_accuracy: 0.7065 - val_loss: 0.8499\n",
      "Epoch 30/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 115ms/step - accuracy: 0.8156 - loss: 0.5109 - val_accuracy: 0.6991 - val_loss: 0.8344\n",
      "Epoch 31/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 117ms/step - accuracy: 0.8173 - loss: 0.5071 - val_accuracy: 0.6938 - val_loss: 0.8588\n",
      "Epoch 32/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 115ms/step - accuracy: 0.8132 - loss: 0.5042 - val_accuracy: 0.6905 - val_loss: 0.8661\n",
      "Epoch 33/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 108ms/step - accuracy: 0.8044 - loss: 0.5172 - val_accuracy: 0.6860 - val_loss: 0.8580\n",
      "Epoch 34/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - accuracy: 0.7772 - loss: 0.6149 - val_accuracy: 0.6905 - val_loss: 0.8732\n",
      "Epoch 35/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 108ms/step - accuracy: 0.8167 - loss: 0.4877 - val_accuracy: 0.7028 - val_loss: 0.8612\n",
      "Epoch 36/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 107ms/step - accuracy: 0.8475 - loss: 0.4330 - val_accuracy: 0.7078 - val_loss: 0.8954\n",
      "Epoch 37/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 105ms/step - accuracy: 0.8367 - loss: 0.4665 - val_accuracy: 0.6954 - val_loss: 0.8963\n",
      "Epoch 38/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 108ms/step - accuracy: 0.8471 - loss: 0.4212 - val_accuracy: 0.6950 - val_loss: 0.9122\n",
      "Epoch 39/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 108ms/step - accuracy: 0.8456 - loss: 0.4293 - val_accuracy: 0.6876 - val_loss: 0.9252\n",
      "Epoch 40/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 105ms/step - accuracy: 0.8524 - loss: 0.3958 - val_accuracy: 0.6930 - val_loss: 0.9372\n",
      "Epoch 41/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 112ms/step - accuracy: 0.8583 - loss: 0.3992 - val_accuracy: 0.6885 - val_loss: 0.9386\n",
      "Epoch 42/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 117ms/step - accuracy: 0.8515 - loss: 0.4051 - val_accuracy: 0.7102 - val_loss: 0.9019\n",
      "Epoch 43/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - accuracy: 0.8644 - loss: 0.3790 - val_accuracy: 0.6991 - val_loss: 0.9409\n",
      "Epoch 44/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 110ms/step - accuracy: 0.8670 - loss: 0.3760 - val_accuracy: 0.6917 - val_loss: 0.9837\n",
      "Epoch 45/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 107ms/step - accuracy: 0.8575 - loss: 0.3770 - val_accuracy: 0.7074 - val_loss: 0.9675\n",
      "Epoch 46/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - accuracy: 0.8781 - loss: 0.3410 - val_accuracy: 0.7152 - val_loss: 0.9295\n",
      "Epoch 47/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 107ms/step - accuracy: 0.8787 - loss: 0.3393 - val_accuracy: 0.7094 - val_loss: 0.9745\n",
      "Epoch 48/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 105ms/step - accuracy: 0.8489 - loss: 0.4162 - val_accuracy: 0.7164 - val_loss: 0.9380\n",
      "Epoch 49/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - accuracy: 0.8663 - loss: 0.3637 - val_accuracy: 0.6942 - val_loss: 1.0073\n",
      "Epoch 50/50\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 108ms/step - accuracy: 0.8658 - loss: 0.3632 - val_accuracy: 0.7090 - val_loss: 0.9620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=True, input_shape=(130, 40))),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.3),\n",
    "    Dense(y_encoded.shape[1], activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "#Save Model\n",
    "model.save(\"lstm_emotion_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎙 Streamlit App\n",
    "st.title(\"🎤 Real-Time Speech Emotion Recognition with LSTM\")\n",
    "\n",
    "# 📌 Function to Record Audio\n",
    "def record_audio(duration=3, fs=44100):\n",
    "    st.write(\"🎙 Recording...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype=\"float32\")\n",
    "    sd.wait()\n",
    "    wavio.write(\"recorded.wav\", recording, fs, sampwidth=2)\n",
    "    st.write(\"✅ Recorded Successfully!\")\n",
    "    return \"recorded.wav\"\n",
    "\n",
    "# 📌 Predict Emotion\n",
    "def predict_emotion(audio_path):\n",
    "    features = extract_features(audio_path)\n",
    "    features = np.expand_dims(features[:130, :], axis=0)  # Ensure shape matches model input\n",
    "    prediction = model.predict(features)\n",
    "    emotion = encoder.inverse_transform([np.argmax(prediction)])[0]\n",
    "    return emotion\n",
    "\n",
    "# 📌 Streamlit UI\n",
    "if st.button(\"🎙 Record and Predict Emotion\"):\n",
    "    recorded_file = record_audio()\n",
    "    predicted_emotion = predict_emotion(recorded_file)\n",
    "    st.subheader(f\"🎭 Detected Emotion: **{predicted_emotion}**\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
